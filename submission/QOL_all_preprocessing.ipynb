{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Quality of Life \n",
    "### DOPP Group 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:38.751792Z",
     "start_time": "2021-01-23T14:10:38.339833Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import eurostat\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Variable Scraper: Quality of Life ranking from Numbeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:38.986485Z",
     "start_time": "2021-01-23T14:10:38.752923Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DEFAULT_YEAR=2020\n",
    "\n",
    "\n",
    "class ReponseVariableScraper:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.year=DEFAULT_YEAR\n",
    "        self.url_base=f\"https://www.numbeo.com/quality-of-life/rankings.jsp?title={self.year}&displayColumn=0\"\n",
    "        self.data={}\n",
    "        self.european_countries=[\"Switzerland\",\"Netherlands\",\"Denmark\",\n",
    "\"Austria\",\"Luxembourg\",\"Iceland\",\"United Kingdom\",\"Germany\",\n",
    "\"Spain\",\"Estonia\",\"Sweden\",\"Ireland\",\"Slovenia\",\"Lithuania\",\n",
    "        \"Turkey\",\"Czech Republic\",\"Norway\",\"Croatia\",\n",
    "        \"France\",\"Belgium\",\"Portugal\",\"Cyprus\",\"Romania\",\"Poland\",\"Slovakia\",\n",
    "        \"Latvia\",\"Russia\",\"Italy\",\"Bulgaria\",\"Serbia\",\"Greece\",\n",
    "\"Hungary\",\"Ukraine\"]\n",
    "\n",
    "    def scrape_year(self,year,path=None):\n",
    "        if year<2012 or year>2020: return\n",
    "        if not os.path.exists(\"../data\"):\n",
    "            os.mkdir(\"../data\")\n",
    "        if not path: path = f\"../data/Quality_of_life_{year}\"\n",
    "\n",
    "        self.year = year\n",
    "        self.url_base = f\"https://www.numbeo.com/quality-of-life/rankings.jsp?title={year}&displayColumn=0\"\n",
    "        if year==2012 or year ==2013:self.url_base = f\"https://www.numbeo.com/quality-of-life/rankings.jsp?title={year}-Q1&displayColumn=0\"\n",
    "\n",
    "        self.__process_content()\n",
    "        self.__save_data(path)\n",
    "\n",
    "    def get_year(self,year,path=None):\n",
    "        if not path: path = f\"../data/Quality_of_life_{year}\"\n",
    "        try:\n",
    "            df=pd.read_csv(path,header=0)\n",
    "        except(FileNotFoundError):\n",
    "            self.scrape_year(year)\n",
    "            df=self.get_year(year)\n",
    "        return df\n",
    "\n",
    "    def __process_content(self):\n",
    "        page = requests.get(self.url_base)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        tb = soup.find(id=\"t2\")\n",
    "        header = tb.find_all_next(\"th\")\n",
    "        headings = []\n",
    "        for head in header:\n",
    "            headings.extend(head.div.contents)\n",
    "        body = tb.find_next(\"tbody\").find_all(\"tr\")\n",
    "        ranks = []\n",
    "        city = []\n",
    "        qol = []\n",
    "        rank = 1\n",
    "        for row in body:\n",
    "            content = row.find_all(\"td\")\n",
    "            ranks.append(rank)\n",
    "            city.extend(content[1])\n",
    "            qol.extend(content[2])\n",
    "            rank+=1\n",
    "        tupls = list(zip(ranks, city, qol))\n",
    "        df = pd.DataFrame(tupls, columns=headings)\n",
    "        cities = df[\"City\"].map(lambda x: x.split(\", \")[0])\n",
    "        countries = df[\"City\"].map(lambda x: x.split(\", \")[1])\n",
    "        df[\"City\"] = cities\n",
    "        df[\"Country\"] = countries\n",
    "        self.data[self.year]=df\n",
    "\n",
    "    def __save_data(self,path):\n",
    "        self.data[self.year].to_csv(path,header=True)\n",
    "    def __filter_europe(self,df):\n",
    "        df=df[df[\"Country\"].isin(self.european_countries)]\n",
    "        return df\n",
    "\n",
    "    def __fill_nan(self,df):\n",
    "        cols = df.columns\n",
    "        n = df.to_numpy().reshape((len(cols),))\n",
    "        x = cols[np.argwhere(~np.isnan(n))]\n",
    "        y = n[np.argwhere(~np.isnan(n))]\n",
    "        reg = LinearRegression().fit(x, y)\n",
    "        miss = cols[np.argwhere(np.isnan(n))]\n",
    "        pred = reg.predict(miss)\n",
    "        n[np.argwhere(np.isnan(n))] = pred\n",
    "        return n\n",
    "    def get_interpolated_years(self):\n",
    "        data = {}\n",
    "        for year in range(2012, 2021):\n",
    "            data[year] = self.get_year(year)\n",
    "            data[year] = self.__filter_europe(data[year])\n",
    "            data[year][\"Rank\"] = range(1, len(data[year]) + 1)\n",
    "        all_cities = []\n",
    "        for year in range(2012, 2021):\n",
    "            all_cities.extend(data[year][\"City\"].unique())\n",
    "        all_cities=list(set(all_cities))\n",
    "        complete_data = pd.DataFrame(\n",
    "            columns=[\"City\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"])\n",
    "        complete_data[\"City\"] = all_cities\n",
    "        for year in range(2012, 2021):\n",
    "            data_year = data[year]\n",
    "            for city in all_cities:\n",
    "                c = data_year[\"City\"].to_list()\n",
    "                if city in c:\n",
    "                    g = data_year[data_year[\"City\"] == city][\"Quality of Life Index\"].to_list()[0]\n",
    "                    indx = complete_data[complete_data[\"City\"] == city]\n",
    "                    indx[str(year)] = float(g)\n",
    "                    complete_data[complete_data[\"City\"] == city] = indx\n",
    "                else:\n",
    "                    complete_data[complete_data[\"City\"] == city][year] = np.nan\n",
    "        res = []\n",
    "        for city in all_cities:\n",
    "            g = complete_data[complete_data[\"City\"] == city]\n",
    "            d = g[[\"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]]\n",
    "            data_year = d\n",
    "            data_year = pd.DataFrame(data_year, dtype='float32')\n",
    "            data_year.interpolate(method='linear', axis=0, inplace=True)\n",
    "            if data_year.isnull().values.any():\n",
    "                data_year = self.__fill_nan(data_year)\n",
    "            else:\n",
    "                data_year = data_year.to_numpy()\n",
    "            fill = pd.DataFrame(columns=[\"City\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"])\n",
    "            fill[\"City\"] = [city]\n",
    "            fill[[\"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]] = data_year\n",
    "            v = fill.values\n",
    "            res.append(v[0])\n",
    "            complete_data[complete_data[\"City\"] == city] = fill\n",
    "        complete_data.head()\n",
    "        g = pd.DataFrame(res, columns=[\"City\", \"2012\", \"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"])\n",
    "        return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:45.031845Z",
     "start_time": "2021-01-23T14:10:38.987694Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "european_countries=[\"Switzerland\",\"Netherlands\",\"Denmark\",\n",
    "\"Austria\",\"Luxembourg\",\"Iceland\",\"United Kingdom\",\"Germany\",\n",
    "\"Spain\",\"Estonia\",\"Sweden\",\"Ireland\",\"Slovenia\",\"Lithuania\",\n",
    "        \"Turkey\",\"Czech Republic\",\"Norway\",\"Croatia\",\n",
    "        \"France\",\"Belgium\",\"Portugal\",\"Cyprus\",\"Romania\",\"Poland\",\"Slovakia\",\n",
    "        \"Latvia\",\"Russia\",\"Italy\",\"Bulgaria\",\"Serbia\",\"Greece\",\n",
    "\"Hungary\",\"Ukraine\"]\n",
    "\n",
    "def filter_europe(df):\n",
    "    df=df[df[\"Country\"].isin(european_countries)]\n",
    "    return df\n",
    "\n",
    "\n",
    "data_scraper = ReponseVariableScraper()\n",
    "\n",
    "for year in range(2012,2021):\n",
    "    data_scraper.scrape_year(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking & Score Aggregation\n",
    "After the necessary preprocessing has been done the next step in our time analysis is to summerize all the dataset into a dictionary containing the response variable, the ranking and city for every year.\n",
    "Furthermore, two a dataframe was created which counts for every city how often it scored in the top 10 by ranking during the time period 2012-2020. Additionally, a dataframe which accumulates for every\n",
    "city the scores that it received over the years. At last both dataframes were sorted by their counts of ranking or accumulated scores in descending order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:46.583980Z",
     "start_time": "2021-01-23T14:10:45.033558Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data={}\n",
    "for year in range(2012,2021):\n",
    "    data[year]=data_scraper.get_year(year)\n",
    "    data[year]=filter_europe(data[year])\n",
    "    data[year][\"Rank\"]=range(1,len(data[year])+1)\n",
    "all_cities=[]\n",
    "for year in range(2012,2021):\n",
    "    all_cities.extend(data[year][\"City\"].unique())\n",
    "\n",
    "all_cities=np.unique(all_cities)\n",
    "cities=pd.DataFrame(columns=[\"City\",\"Top_10\"])\n",
    "cities[\"City\"]=all_cities\n",
    "cities[\"Top_10\"]=0\n",
    "QOL_acc=pd.DataFrame(columns=[\"City\",\"QOL_acc\"])\n",
    "QOL_acc[\"City\"]=all_cities\n",
    "QOL_acc[\"QOL_acc\"]=0\n",
    "for year in range(2012,2021):\n",
    "    df=data[year]\n",
    "    df.sort_values(by=\"City\",inplace=True)\n",
    "    candidates=df[df[\"Rank\"]<=10][\"City\"].to_list()\n",
    "    idx_c=cities[\"City\"].isin(candidates)\n",
    "    cities[\"Top_10\"].loc[idx_c]+=1\n",
    "    c=df[\"City\"].to_list()\n",
    "    idx_acc=QOL_acc[\"City\"].isin(c)\n",
    "    q=df[\"Quality of Life Index\"].to_list()\n",
    "    QOL_acc[\"QOL_acc\"][idx_acc]+=q\n",
    "cities.sort_values(by=[\"Top_10\"],ascending=False,inplace=True)\n",
    "qol_response=data_scraper.get_interpolated_years()\n",
    "#qol_response.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:46.592976Z",
     "start_time": "2021-01-23T14:10:46.586271Z"
    }
   },
   "outputs": [],
   "source": [
    "##reshape (melt) into city-year : value shape, convert year to int to match others\\n\",\n",
    "\n",
    "response = qol_response.melt(id_vars=['City'], value_vars=['2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012'],\n",
    "                         var_name = 'Year', value_name='QOL')\n",
    "response['Year'] = response['Year'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure below the dataframe which depicts the number of times a city scored in the top 10 is visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate: Cooling and heating degree days by NUTS 2 regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:47.274174Z",
     "start_time": "2021-01-23T14:10:46.593975Z"
    }
   },
   "outputs": [],
   "source": [
    "## get eurostat data\n",
    "heatdays = eurostat.get_data_df('nrg_chddr2_a', flags=False)\n",
    "heatdays = heatdays.iloc[:,:11].rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "## Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "target_cities = pd.read_csv(\"../data/Cities_with_codes.csv\")\n",
    "heatdays_nuts = pd.merge(target_cities, heatdays, on=[' NUTS 2'], how='left')\n",
    "\n",
    "## split into cool & heating days\n",
    "heatdays_cdd = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg == 'CDD')]\n",
    "heatdays_hdd = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg == 'HDD')]\n",
    "\n",
    "# impute missing heat days\n",
    "missing_heat = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg.isna())]\n",
    "missing_heat['indic_nrg'] = 'HDD'\n",
    "for year in range(2012, 2020):\n",
    "    missing_heat[year] = heatdays_hdd[year].mean(axis=0)\n",
    "heatdays_hdd = heatdays_hdd.append(missing_heat)\n",
    "\n",
    "## impute missing cool days\n",
    "missing_cool = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg.isna())]\n",
    "missing_cool['indic_nrg'] = 'CDD'\n",
    "for year in range(2012, 2020):\n",
    "    missing_cool[year] = heatdays_cdd[year].mean(axis=0)\n",
    "heatdays_cdd = heatdays_cdd.append(missing_cool)\n",
    "\n",
    "## reshape (melt) into city-year : value shape\n",
    "heat = heatdays_hdd.melt(id_vars=['City'], value_vars=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012],\\\n",
    "                         var_name = 'Year', value_name='Heatdays')\n",
    "cool = heatdays_cdd.melt(id_vars=['City'], value_vars=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012],\\\n",
    "                         var_name = 'Year', value_name='Cooldays')\n",
    "heat.shape\n",
    "#heatdays_cdd.to_csv('../data/heatdays_hdd.csv')\n",
    "#heatdays_hdd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scrape Pollution from Numbeo using modified version of Response Scraper\n",
    "!python Pollution_Scraper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:53.903687Z",
     "start_time": "2021-01-23T14:10:47.275227Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## for each year: read QOL + pollution csv, merge, add year, append to combined df \n",
    "pollution = pd.DataFrame()    \n",
    "for year in range(2012,2020):\n",
    "    ## read pollution for year\n",
    "    filename = ('../data/' + 'Pollution_' + str(year))\n",
    "    df_yr = pd.read_csv(filename).iloc[:,2:4]\n",
    "\n",
    "    ## define year and rename\n",
    "    df_yr.loc[:,'Year'] = int(year)\n",
    "    df_yr = df_yr.rename(columns = {'Pollution Index':'Pollution','Quality of Life Index':'QOL'})\n",
    "    \n",
    "    ## concat to combined df\n",
    "    pollution = pd.concat([pollution, df_yr])\n",
    "    \n",
    "## reshape & interpolate missing values\n",
    "pollution_filled = pollution.pivot_table(index = 'City', columns = 'Year')\n",
    "pollution_filled = pollution_filled.interpolate(method='linear', axis=1, limit_direction='both')\n",
    "\n",
    "## merge back & fill missing city (Ankara)\n",
    "pollution_merge = pd.merge(target_cities, pollution_filled, on=['City'], how='left').fillna(50)\n",
    "\n",
    "## rename columens, merge back\n",
    "renamed = pollution_merge.iloc[:,4:].rename(columns = lambda name: int(str(name).split()[1][0:4]))\n",
    "city_id = pollution_merge.iloc[:,0:1]\n",
    "pollution_named = pd.concat([renamed, city_id], axis=1)\n",
    "\n",
    "## melt to right shape\n",
    "pollution = pollution_named.melt(id_vars=['City'], value_vars=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012],\\\n",
    "                           var_name = 'Year', value_name='Pollution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education Attainment\n",
    "From the eurostat dataset on education Population by educational attainment level (edat1) we use the sub-data set Population aged 25-64 by educational attainment level, sex and NUTS 2 regions (%) (edat_lfse_04). With educational attainment meaning the\n",
    "highest level of completed education, the data is divided into the three sub categories ED 0-2 (less than primary - primary education), ED 3-4 (lower secondary - post secondary education) and ED 5-8 (tertiary education i.e. university studies). Each city\n",
    "is given the values of the corresponding NUTS 2 region.\n",
    "\n",
    "Every city from the target variables are found represented in the\n",
    "data, however a few (['Belgrade', 'Budapest', 'Edinburgh', 'Kaunas', 'Vilnius', 'Warsaw']) have missing values for 2012.\n",
    "Since this data is the earliest value it is not possible to interpolate. One option would be to use the later data points as a baseline\n",
    " for linear extrapolation, however because of the few datapoints and relatively high variance between the values this\n",
    " gave results with quite high variation, it is then more likely to guess a stronger trend than exists. Therefore we simply pad with the 2013 value since the\n",
    "  value is most likely to be close to the coming year and also should be closer than using the mean of all the years.\n",
    "Furthermore the data from Glasgow was almost completely missing except for 2012, but since Glasgow is a relatively large city in Scotland the population is close to 33% percent of the population\n",
    "of the whole of Scotland (the NUTS 1 region) which is available we fill it with these values as an approximation.\n",
    "\n",
    "Finally we separate the data into the three different brackets of educational level and drop the data for the total levels. This is since it should be\n",
    "a significant difference especially between the primary/secondary and the tertiary education since in general the first to\n",
    "are more heavliy regulated and handled on a governmental/regional compared to the Universities and similiar that usually are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:54.478310Z",
     "start_time": "2021-01-23T14:10:53.905775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the data from Eurostat\n",
    "df = eurostat.get_data_df('edat_lfse_04')\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df = df.drop(df.loc[:, '2011': '2000'].columns, axis = 1)\n",
    "df = df.drop(['unit', 'age'], axis = 1)\n",
    "\n",
    "# Keep only total values and discard the gender column\n",
    "df = df[df.sex == 'T']\n",
    "df = df.drop('sex', axis = 1)\n",
    "\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "#target_cities = pd.read_csv(\"../data/Cities_with_codes.csv\")\n",
    "education_attainment = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = education_attainment.loc[education_attainment['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow (Metro area) is approx 33% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "education_attainment[education_attainment['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "\n",
    "# Impute the missing values from 2012 using padding\n",
    "education_attainment.loc[:, '2019':'2012'] = education_attainment.loc[:, '2019':'2012'].fillna(method='pad',axis=1)\n",
    "\n",
    "# Split in to 3 separate tables depending on education level\n",
    "edu_ED_0_2 = education_attainment[education_attainment['isced11'] == 'ED0-2']\n",
    "edu_ED_3_4 = education_attainment[education_attainment['isced11'] == 'ED3_4']\n",
    "edu_ED_5_8 = education_attainment[education_attainment['isced11'] == 'ED5-8']\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "education_attainment  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= edu_ED_0_2[ list(edu_ED_0_2.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Education Attainment ED 0-2\"})\n",
    "    yearly_data.insert(6, \"Education Attainment ED 3-4\", edu_ED_3_4[f\"{year}\"].to_numpy())\n",
    "    yearly_data.insert(7, \"Education Attainment ED 5-8\", edu_ED_5_8[f\"{year}\"].to_numpy())\n",
    "    education_attainment = education_attainment.append(yearly_data)\n",
    "    \n",
    "education_attainment = education_attainment.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Working Hours\n",
    "\n",
    "From the eurostat data Regional labour market statistics (reg_lmk) we use the dataset Average number of usual weekly hours of work in main job by sex, age and NUTS 2 regions (lfst_r_lfe2ehour), which contains data\n",
    "of working hours for a wide variety of different age categories and gender. We are however only interested in a total average\n",
    "for both sexes and all ages so we only use the largest agespan (15-74).\n",
    "\n",
    "The data is used as a measure of how much the average person\n",
    " needs to work every week which is clearly connected to the amount of leisure time which should have a large impact on the quality of life. The missing data is missing in the exact same spots as\n",
    "in the educational attainment data so we use the same approach here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:55.038215Z",
     "start_time": "2021-01-23T14:10:54.479565Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the datasheet from eurostat\n",
    "df = eurostat.get_data_df('lfst_r_lfe2ehour')\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df.columns = df.columns.astype(str)\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Extract the data for both sexes (Total) and age group 15-74\n",
    "df = df[df.sex == 'T']\n",
    "df = df.drop('sex', axis = 1)\n",
    "df = df[df.age == 'Y15-74']\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "avg_hours = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = avg_hours.loc[avg_hours['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow is 40% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "avg_hours[avg_hours['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "# Impute the rest using padding\n",
    "#avg_hours = avg_hours.interpolate(method='pad',axis=1)\n",
    "avg_hours.loc[:, '2019':'2012'] = avg_hours.loc[:, '2019':'2012'].fillna(method='pad',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "avg_working_hours  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= avg_hours[ list(avg_hours.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Average Working Hours\"})\n",
    "    avg_working_hours = avg_working_hours.append(yearly_data)\n",
    "avg_working_hours = avg_working_hours.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment Rate\n",
    "\n",
    "From the eurostat data Regional labour market statistics (reg_lmk) we use the dataset \tUnemployment rates by sex, age and NUTS 2 regions (lfst_r_lfu3rt).\n",
    "Once again we are only interested in total statistics for the genders and the largest age-group and therefore use only the entries with sex 'T' and agespan 15-74.\n",
    "Also once again the same data is missing (likely these were all reported/obtained in the same survey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:55.636772Z",
     "start_time": "2021-01-23T14:10:55.039396Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read the datasheet from Eurostat\n",
    "df = eurostat.get_data_df('lfst_r_lfu3rt')\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df.columns = df.columns.astype(str)\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Extract the data for both sexes (Total) and age group 15-74\n",
    "df = df[df.sex == 'T']\n",
    "df = df[df.age == 'Y15-74']\n",
    "df = df.drop(['sex','unit','age'], axis = 1)\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "unemployment_rate = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = unemployment_rate.loc[unemployment_rate['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow is 40% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "unemployment_rate[unemployment_rate['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "# Impute the rest using padding\n",
    "#unemployment_rate = unemployment_rate.interpolate(method='pad',axis=1)\n",
    "unemployment_rate.loc[:, '2019':'2012'] = unemployment_rate.loc[:, '2019':'2012'].fillna(method='pad',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "unemployment_rate_data  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= unemployment_rate[ list(unemployment_rate.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Unemployment_Rate\"})\n",
    "    unemployment_rate_data = unemployment_rate_data.append(yearly_data)\n",
    "unemployment_rate_data = unemployment_rate_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Victims in Road Accidents\n",
    "\n",
    "From the eurostat data Regional transport statistics (reg_tran) we use the dataset \tVictims in road accidents by NUTS 2 regions (tran_r_acc).\n",
    "The dataset contains data on a NUTS 2 level for reported road accidents. There are different measures availible, but we use P_MHAB (Per million inhabitants)\n",
    "as it gives a good way to compare the cities (with a wide variety of different sizes) to each other. There are also two different\n",
    "variables of reported accident: accidents with injuries or accidents with lethal outcome. Even though the injury data might\n",
    "be a better measure of the road safety in a city there is more data missing and hence we decided on the accidents with lethal outcome. Also the\n",
    "accidents with injueries there might be a larger dependency on willingness to report crimes than in the lethal accidents\n",
    "which are more likely very highly reported.\n",
    "\n",
    "Here there is also more data missing, there is no data at all for the cities\n",
    "Reykjavik, Edinburgh, Glasgow and Belgrade. Traffic data for these regions were obtained by searching up other data sources\n",
    "such as governmental reports from the corresponding countries and then approximated to the city. Also there is no data\n",
    "available at all for the year 2019 for which we pad the values from 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.036296Z",
     "start_time": "2021-01-23T14:10:55.637763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the data from Eurostat\n",
    "df = eurostat.get_data_df('tran_r_acci')\n",
    "df.columns = df.columns.astype(str)\n",
    "# Drop all years before 2012, keep only with unit measure Per Million Inhabitants and accident type deadly\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "df = df[df.unit == 'P_MHAB']\n",
    "df = df[df.victim == 'KIL']\n",
    "\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Insert missing cloumn for 2019\n",
    "df.insert(3,\"2019\",np.NaN)\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "road_accidents = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Check for missing cities\n",
    "missing_cities = target_cities[-target_cities[' NUTS 2'].isin(road_accidents[' NUTS 2'])]\n",
    "\n",
    "\n",
    "print(\"Missing cities: \", missing_cities.values[:,0])\n",
    "\n",
    "# Data from Icelandic ministry of transportation\n",
    "#https://www.samgongustofa.is/umferd/tolfraedi/slysatolur/arsskyrslur-slysaskraningar/\n",
    "# Reported as deaths per 10,000\n",
    "Reykjavik  = [0.0, 30.0, 20.0, 10.0, 20.0, 0.0, 10.0, 10.0]\n",
    "Reykjavik = ['Reykjavik', 'IS001C1','IS00','IS','KIL','P_MHAB']+Reykjavik\n",
    "road_accidents.loc[77] = Reykjavik\n",
    "\n",
    "# Table 1-1 in : https://www.abs.gov.rs/admin/upload/documents/20181016102533-statistical_report_2016_english.pdf\n",
    "# 20% of the population is in Belgrade so we take 20% of the accidents as occuring there\n",
    "# and divide by 1.7 (million inhabitants)\n",
    "Belgrade = [np.NaN, np.NaN,np.NaN, 619, 594, 476, 548, 551]\n",
    "Belgrade = np.multiply(Belgrade,(0.2/1.7))\n",
    "Belgrade = np.around(Belgrade, 1)\n",
    "Belgrade = ['Belgrade', '-','RS11','RS','KIL','P_MHAB']+Belgrade.tolist()\n",
    "road_accidents.loc[78] = Belgrade\n",
    "\n",
    "# Traffic death data from Scotland\n",
    "# https://statistics.gov.scot/data/road-safety\n",
    "# Select by region\n",
    "\n",
    "# Divide by 0.5 (million inhabitants)\n",
    "Edinburgh = [np.NaN, 5, 6, 9, 3, 11, 8, 13]\n",
    "Edinburgh = np.divide(Edinburgh,0.5)\n",
    "Edinburgh = np.around(Edinburgh, 1)\n",
    "Edinburgh = ['Edinburgh', 'UK007C1','UKM7','UK','KIL','P_MHAB']+Edinburgh.tolist()\n",
    "road_accidents.loc[79] = Edinburgh\n",
    "\n",
    "# Divide by 0.6 (million inhabitants)\n",
    "Glasgow = [np.NaN, 10, 7, 8, 15, 18, 4, 7]\n",
    "Glasgow = np.divide(Glasgow,0.6)\n",
    "Glasgow = np.around(Glasgow, 1)\n",
    "Glasgow = ['Glasgow', 'UK004C1','UKM3','UK','KIL','P_MHAB']+Glasgow.tolist()\n",
    "road_accidents.loc[80] = Glasgow\n",
    "\n",
    "# Impute using padding to fill missing values\n",
    "road_accidents.loc[:, '2019':'2013'] = road_accidents.loc[:, '2019':'2012'].fillna(method='backfill',axis=1)\n",
    "road_accidents.loc[:, '2019':'2012'] = road_accidents.loc[:, '2019':'2012'].fillna(method='ffill',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "deaths_in_road_accidents  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= road_accidents[ list(road_accidents.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Deaths_in_road_accidents\"})\n",
    "    deaths_in_road_accidents = deaths_in_road_accidents.append(yearly_data)\n",
    "deaths_in_road_accidents = deaths_in_road_accidents.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Education, Unemployemnt Rate, Accidents & Working Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.055016Z",
     "start_time": "2021-01-23T14:10:56.037227Z"
    }
   },
   "outputs": [],
   "source": [
    "data_sheets = [deaths_in_road_accidents,avg_working_hours,unemployment_rate_data]\n",
    "\n",
    "initial = education_attainment\n",
    "merged = initial\n",
    "\n",
    "for file_name in data_sheets:\n",
    "    temp_dataframe = file_name\n",
    "    values = temp_dataframe.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "    merged = merged.merge(values, on=['City', 'Year'])\n",
    "    \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "    \n",
    "print(merged.shape)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health and Gender\n",
    "\n",
    "Unfortunately, parts of the health data could not be retrieved using the scrapper. We thus downloaded it to a .csv file manually from the website (https://ec.europa.eu/eurostat/databrowser/view/hlth_rs_prsrg/default/table?lang=en). On a NUTS2 level the health variables available are life expectancy and medical staff. The later is subdivided (There seems to be the problem with the scrapper.) into absolute and relative number as well as certain classes of personnel: Dentists, medical doctors, midwifes, etc. We chose the variable 'medical doctors' and decided to use the relative number per 100.000 inhabitants as the observed cities vary largely in population. Pre-processing has been necessary for missing values. First, linear interpolation has been used to calculate single missing values. Second, if consecutive values have been missing a simple regression has been applied to calculate the missing values. Third, if no data has been available for a given city at all, we had to resort to the next more crude granularity to use that data instead which had to be applied for example for Warsaw. Last, for all cities of the United Kingdom and Ireland data on medical personnel has neither been reported on a NUTS2 nor on a NUTS1 level. We thus had to use national data to fill in the gaps. As a result all English, Scottish and northern Irish have the same number of medical doctors per 100.000 inhabitants. The same goes for the Irish cities Cork and Dublin. \n",
    "\n",
    "Additionally the variable 'gender unemployment gap' has been chosen to approximate the level of equality. It is the difference between male and female unemployment rate and monitors female labor market access. It should be corrected for overall unemployment rate to take economic development into consideration. So this might not be the best variable to measure gender inequality, but it is the only one on a regional level we could find. The data has been available for all cities except for Belgrade which we approximated using the average of the neighboring capitals Budapest, Bucharest and Sofia for each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.145159Z",
     "start_time": "2021-01-23T14:10:56.055941Z"
    }
   },
   "outputs": [],
   "source": [
    "health_gender = pd.read_csv('../data/Cities_Health_Gender.csv')\n",
    "health_gender = health_gender.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "#health_gender.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:11:36.727032Z",
     "start_time": "2021-01-23T14:11:36.708872Z"
    }
   },
   "outputs": [],
   "source": [
    "join = 'inner'\n",
    "#join = 'outer'\n",
    "\n",
    "heatcool = pd.merge(heat, cool, how=join, on=['City','Year'] )\n",
    "print(heatcool.shape)\n",
    "\n",
    "heatcoolpol = pd.merge(heatcool, pollution, how=join, on=['City','Year'] )\n",
    "print(heatcoolpol.shape)\n",
    "\n",
    "tmppolres = pd.merge(heatcoolpol, response , how=join, on=['City','Year'] )\n",
    "print(tmppolres.shape)\n",
    "tmppolres.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:11:38.193171Z",
     "start_time": "2021-01-23T14:11:38.182142Z"
    }
   },
   "outputs": [],
   "source": [
    "halfdata = pd.merge(tmppolres, merged , how=join, on=['City','Year'] )\n",
    "halfdata = halfdata.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "print(halfdata.shape)\n",
    "#halfdata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:11:38.635964Z",
     "start_time": "2021-01-23T14:11:38.619254Z"
    }
   },
   "outputs": [],
   "source": [
    "alldata = pd.merge(halfdata, health_gender, how=join, on=['City','Year'] )\n",
    "print(alldata.shape)\n",
    "alldata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:11:43.136798Z",
     "start_time": "2021-01-23T14:11:43.121550Z"
    }
   },
   "outputs": [],
   "source": [
    "## inner shape: (376, 15), outer: (1072, 15) \n",
    "if join == 'inner':\n",
    "    alldata.to_csv('../data/Full_data_inner.csv', index = False)\n",
    "elif join == 'outer':\n",
    "    alldata.to_csv('../data/Full_data_outer.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:11:45.092957Z",
     "start_time": "2021-01-23T14:11:45.084249Z"
    }
   },
   "outputs": [],
   "source": [
    "all_inner = pd.read_csv('../data/Full_data_inner.csv')\n",
    "all_inner.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.149885Z",
     "start_time": "2021-01-23T14:10:38.374Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(x=cities.head(10)[\"City\"],height=cities.head(10)[\"Top_10\"])\n",
    "plt.title(\"Top 10 ratings between 2012 and 2020\")\n",
    "plt.ylabel(\"Times ranked in Top 10\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.savefig(\"Top_10_rankings\",bbox_inchesstr=\"tight\",pad_inches=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the top 10 cities by ranking are visualized with their respective ranks over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.150519Z",
     "start_time": "2021-01-23T14:10:38.377Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_10=cities.head(10)\n",
    "print(top_10)\n",
    "time_dependency_top_10={}\n",
    "for city in top_10[\"City\"]:\n",
    "    time_dependency_top_10[city]=[]\n",
    "    for year in range(2012,2021):\n",
    "        d=data[year]\n",
    "        time_dependency_top_10[city].extend(d[d[\"City\"]==city][\"Rank\"].to_list())\n",
    "plt.figure(figsize=(10,15))\n",
    "for time in time_dependency_top_10.keys():\n",
    "    p=time_dependency_top_10[time]\n",
    "    plt.plot(time_dependency_top_10[time])\n",
    "    for j,i in zip(time_dependency_top_10[time],range(0,9)):\n",
    "            plt.text(i,j,str(j))\n",
    "plt.ylim(35,0)\n",
    "plt.xticks(np.arange(9),(\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))\n",
    "plt.yticks((1,4,8,12,16,20,24,28,32,36))\n",
    "plt.ylabel(\"Rank\")\n",
    "plt.title(\"Time developement of ranking of top 10 cities\")\n",
    "plt.legend(top_10[\"City\"])\n",
    "plt.savefig(\"Top_10_rankings_ranks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further clarity the same time developement was done for the scores of the top 10 cities by ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.151135Z",
     "start_time": "2021-01-23T14:10:38.379Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for city in top_10[\"City\"]:\n",
    "    time_dependency_top_10[city]=[]\n",
    "    for year in range(2012,2021):\n",
    "        d=data[year]\n",
    "        time_dependency_top_10[city].extend(d[d[\"City\"]==city][\"Quality of Life Index\"].to_list())\n",
    "plt.figure(figsize=(10,20))\n",
    "for time in time_dependency_top_10.keys():\n",
    "    p=time_dependency_top_10[time]\n",
    "    plt.plot(time_dependency_top_10[time])\n",
    "    for j,i in zip(time_dependency_top_10[time],range(0,9)):\n",
    "            plt.text(i,j,str(j))\n",
    "plt.xticks(np.arange(9),(\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Time developement of score of top 10 cities\")\n",
    "plt.legend(top_10[\"City\"])\n",
    "plt.savefig(\"Top_10_scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores Margin\n",
    "As it was quite visible from the two graphs above that the rankings and scores do not move in aligned with eachother the next plot shows how the standard deviation changes over time.\n",
    "From the plot below it can be derived that the difference between two scores which are attributed to two neighboring rankings is become smaller over time. This means that the margin between\n",
    "scores is become thinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.151786Z",
     "start_time": "2021-01-23T14:10:38.381Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "std=[]\n",
    "for year in range(2012,2021):\n",
    "    std.append(np.std(data[year][\"Quality of Life Index\"]))\n",
    "\n",
    "plt.plot(std)\n",
    "plt.xticks(np.arange(9),(\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))\n",
    "plt.title(\"Time developement of standard deviation\")\n",
    "plt.ylabel(\"Standard deviation in scores\")\n",
    "plt.savefig(\"Standard deviation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we looked at the 10 cities which scored the highest accumulated scores over time. It can be seen that while the top candidates stayed the same the lower end was replaced by different cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.152441Z",
     "start_time": "2021-01-23T14:10:38.384Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "score_top_10=QOL_acc.sort_values(by=\"QOL_acc\",ascending=False).head(10)\n",
    "score_top_10.head()\n",
    "plt.bar(x=score_top_10[\"City\"],height=score_top_10[\"QOL_acc\"])\n",
    "plt.ylabel(\"Accumulated Score over 10 years\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.savefig(\"top10_scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.153040Z",
     "start_time": "2021-01-23T14:10:38.386Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "time_dependency_score_acc={}\n",
    "for city in score_top_10[\"City\"]:\n",
    "    time_dependency_score_acc[city]=[]\n",
    "    for year in range(2012,2021):\n",
    "        d=data[year]\n",
    "        time_dependency_score_acc[city].extend(d[d[\"City\"]==city][\"Rank\"].to_list())\n",
    "plt.figure(figsize=(10,15))\n",
    "for time in time_dependency_score_acc.keys():\n",
    "    p=time_dependency_score_acc[time]\n",
    "    plt.plot(time_dependency_score_acc[time])\n",
    "    for j,i in zip(time_dependency_score_acc[time],range(0,9)):\n",
    "            plt.text(i,j,str(j))\n",
    "plt.ylim(35,0)\n",
    "plt.xticks(np.arange(9),(\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"))\n",
    "plt.yticks((1,4,8,12,16,20,24,28,32,36))\n",
    "plt.ylabel(\"Rank\")\n",
    "plt.title(\"Time developement of ranking of top 10 cities by accumulated score\")\n",
    "plt.legend(score_top_10[\"City\"])\n",
    "plt.savefig(\"top10_scores_scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the connection between the variables collected and the Quality of Life using a simple linear regression model profided by the sklean package. We refrain from suggesting any causality but note that the effect on the Quality of Life can be explained rather well using certain variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.153671Z",
     "start_time": "2021-01-23T14:10:38.388Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, max_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.154285Z",
     "start_time": "2021-01-23T14:10:38.389Z"
    }
   },
   "outputs": [],
   "source": [
    "# define functions to calculate and print performance metrics\n",
    "def print_metrics(groundtruth, predictions):\n",
    "    print(\"Root Mean Squared Error: %.4f\" % np.sqrt(mean_squared_error(groundtruth, predictions)))\n",
    "    print(\"Mean Absolute Error: %.4f\" % mean_absolute_error(groundtruth, predictions))\n",
    "    print(\"Max Error: %.4f\" % max_error(groundtruth, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.154871Z",
     "start_time": "2021-01-23T14:10:38.392Z"
    }
   },
   "outputs": [],
   "source": [
    "all_inner = pd.read_csv('../data/Full_data_inner.csv')\n",
    "print(all_inner.shape)\n",
    "all_inner.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data collected preciously into a target variables (Quality of Life) as well as into a train and a test set. The later split uses 75% for the train set and the remainder for the testing purpose. Also we have to omit the varibale Year as its numerical nature would distort the share of variance explained by other (reasonable) variables. The variable City has also to be omitted as the model estimation exploits the mechanical calculus of the ordinary least square approach which could not be performed with non numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.155487Z",
     "start_time": "2021-01-23T14:10:38.394Z"
    }
   },
   "outputs": [],
   "source": [
    "# split target variable from data\n",
    "train_data = all_inner.drop(['City', 'Year', 'QOL'], axis=1)\n",
    "train_values = all_inner.loc[:,'QOL']\n",
    "\n",
    "# split data into training and test set\n",
    "train_data, test_data, train_values, test_values = model_selection.train_test_split(\n",
    "    train_data, train_values, random_state=20210123\n",
    ")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for the ordinary least squares regression is taken from the sklean package. So are all further methods like the predict and fit functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.156153Z",
     "start_time": "2021-01-23T14:10:38.395Z"
    }
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(train_data, train_values)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "pred = regr.predict(test_data)\n",
    "\n",
    "# Print metrics\n",
    "print_metrics(test_values, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the root mean squared error and the mean absolute error are not too far off making up 20.58% and 15.37% of the mean quality of life which equals 149.44. This suggests that the set of variables collected is rather useful to explain the Quality of life of the observed cities and years.\n",
    "Some outliers of course exist. The most drastic one being Riga in 2013 where the predicted Quality of life has been grossly overestimated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.156738Z",
     "start_time": "2021-01-23T14:10:38.398Z"
    }
   },
   "outputs": [],
   "source": [
    "regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T14:10:56.157365Z",
     "start_time": "2021-01-23T14:10:38.399Z"
    }
   },
   "outputs": [],
   "source": [
    "coefficents = pd.DataFrame(regr.coef_)\n",
    "coefficents = coefficents.T\n",
    "coefficents.columns = list(train_data.columns) \n",
    "coefficents.index = ['all Years']\n",
    "coefficents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data frame shows the fitted coefficients for the ordinary least squares linear regression model applied. Even though we would not expect to have overfitted the training data (486 cases and 12 variables) we see unexpected results which might stem from too little variance within the data or too much correlation between variables and target. While the positive coefficient of all education variables are easy to explain, we find it rather surpising that cool weather positively correlates with quality of life while hot weather correlates negatively. On the other hand, \"metrics of unfortunate events\" like death in road accidents, unemployment rate, and average working hours show negative effect on quality of life suggesting that fewer road accidents and working hours as well as a lower unemployment rate might increase the quality of life. A possibility for further research presents the partialy contradictory effects of working hours, unemployment rate and gender employment gap of which the later conflicts with our expectation that quality of life profits from decreased inequality. The reason for this conflict might be an overfitting of the variance through these tightly linked variables. The same might be true for the effect of like expectancy an the relative number of doctors per 100,000 inhabitants. Naturally, we would have expected the quality of life to increase with the number of doctors. However, the reason for the suggests negative correlation between doctors and quality of life might be the over fitting of variance in using life expectancy and number of doctors in the same model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
