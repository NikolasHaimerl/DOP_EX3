{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Quality of Life \n",
    "### DOPP Group 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import eurostat\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "#pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response: Quality of Life ranking from Numbeo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Response_Variable import ReponseVariableScraper\n",
    "\n",
    "european_countries=[\"Switzerland\",\"Netherlands\",\"Denmark\",\n",
    "\"Austria\",\"Luxembourg\",\"Iceland\",\"United Kingdom\",\"Germany\",\n",
    "\"Spain\",\"Estonia\",\"Sweden\",\"Ireland\",\"Slovenia\",\"Lithuania\",\n",
    "        \"Turkey\",\"Czech Republic\",\"Norway\",\"Croatia\",\n",
    "        \"France\",\"Belgium\",\"Portugal\",\"Cyprus\",\"Romania\",\"Poland\",\"Slovakia\",\n",
    "        \"Latvia\",\"Russia\",\"Italy\",\"Bulgaria\",\"Serbia\",\"Greece\",\n",
    "\"Hungary\",\"Ukraine\"]\n",
    "\n",
    "def filter_europe(df):\n",
    "    df=df[df[\"Country\"].isin(european_countries)]\n",
    "    return df\n",
    "\n",
    "\n",
    "data_scraper = ReponseVariableScraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking & Score Aggregation\n",
    "After the necessary preprocessing has been done the next step in our time analysis is to summerize all the dataset into a dictionary containing the response variable, the ranking and city for every year.\n",
    "Furthermore, two a dataframe was created which counts for every city how often it scored in the top 10 by ranking during the time period 2012-2020. Additionally, a dataframe which accumulates for every\n",
    "city the scores that it received over the years. At last both dataframes were sorted by their counts of ranking or accumulated scores in descending order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/workspace/DOP_EX3/submission/Response_Variable.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  indx[str(year)] = float(g)\n",
      "/workspace/DOP_EX3/submission/Response_Variable.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  complete_data[complete_data[\"City\"] == city][year] = np.nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stockholm</td>\n",
       "      <td>166.050003</td>\n",
       "      <td>175.729996</td>\n",
       "      <td>160.860001</td>\n",
       "      <td>174.119995</td>\n",
       "      <td>180.509995</td>\n",
       "      <td>160.240005</td>\n",
       "      <td>164.160004</td>\n",
       "      <td>166.449997</td>\n",
       "      <td>162.520004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City        2012        2013        2014        2015        2016  \\\n",
       "0  Stockholm  166.050003  175.729996  160.860001  174.119995  180.509995   \n",
       "\n",
       "         2017        2018        2019        2020  \n",
       "0  160.240005  164.160004  166.449997  162.520004  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data={}\n",
    "for year in range(2012,2021):\n",
    "    data[year]=data_scraper.get_year(year)\n",
    "    data[year]=filter_europe(data[year])\n",
    "    data[year][\"Rank\"]=range(1,len(data[year])+1)\n",
    "all_cities=[]\n",
    "for year in range(2012,2021):\n",
    "    all_cities.extend(data[year][\"City\"].unique())\n",
    "\n",
    "all_cities=np.unique(all_cities)\n",
    "cities=pd.DataFrame(columns=[\"City\",\"Top_10\"])\n",
    "cities[\"City\"]=all_cities\n",
    "cities[\"Top_10\"]=0\n",
    "QOL_acc=pd.DataFrame(columns=[\"City\",\"QOL_acc\"])\n",
    "QOL_acc[\"City\"]=all_cities\n",
    "QOL_acc[\"QOL_acc\"]=0\n",
    "for year in range(2012,2021):\n",
    "    df=data[year]\n",
    "    df.sort_values(by=\"City\",inplace=True)\n",
    "    candidates=df[df[\"Rank\"]<=10][\"City\"].to_list()\n",
    "    idx_c=cities[\"City\"].isin(candidates)\n",
    "    cities[\"Top_10\"].loc[idx_c]+=1\n",
    "    c=df[\"City\"].to_list()\n",
    "    idx_acc=QOL_acc[\"City\"].isin(c)\n",
    "    q=df[\"Quality of Life Index\"].to_list()\n",
    "    QOL_acc[\"QOL_acc\"][idx_acc]+=q\n",
    "cities.sort_values(by=[\"Top_10\"],ascending=False,inplace=True)\n",
    "\n",
    "qol_response = data_scraper.get_interpolated_years()\n",
    "qol_response.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Year</th>\n",
       "      <th>QOL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stockholm</td>\n",
       "      <td>2020</td>\n",
       "      <td>162.520004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City  Year         QOL\n",
       "0  Stockholm  2020  162.520004"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reshape (melt) into city-year : value shape, convert year to int to match others\n",
    "response = qol_response.melt(id_vars=['City'], value_vars=['2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012'],\\\n",
    "                         var_name = 'Year', value_name='QOL')\n",
    "response['Year'] = response['Year'].astype(int)\n",
    "response.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Pollution_Scraper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Pollution</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bucharest</td>\n",
       "      <td>140.71</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City  Pollution  Year\n",
       "0  Bucharest     140.71  2012"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for each year: read QOL + pollution csv, merge, add year, append to combined df \n",
    "pollution = pd.DataFrame()    \n",
    "for year in range(2012,2021):\n",
    "    ## read pollution for year\n",
    "    filename = ('../data/' + 'Pollution_' + str(year))\n",
    "    df_yr = pd.read_csv(filename).iloc[:,2:4]\n",
    "    \n",
    "    ## read Quality of Life for year\n",
    "    #filename_qol = ('../../data/' + 'Quality_of_life_' + str(year))\n",
    "    #df_qol = pd.read_csv(filename_qol).iloc[:,2:4]\n",
    "    \n",
    "    # merging QOL & Pollution \n",
    "    #df_yr = pd.merge(df_yr,qol_y, how = 'outer', on = 'City')\n",
    "    \n",
    "    ## define year and rename\n",
    "    df_yr.loc[:,'Year'] = int(year)\n",
    "    df_yr = df_yr.rename(columns = {'Pollution Index':'Pollution','Quality of Life Index':'QOL'})\n",
    "    \n",
    "    ## concat to combined df\n",
    "    pollution = pd.concat([pollution, df_yr])\n",
    "\n",
    "#df_all.to_csv('../data/Pol_merged.csv', index = False)\n",
    "pollution.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate: Cooling and heating degree days by NUTS 2 regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(488, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get eurostat data\n",
    "heatdays = eurostat.get_data_df('nrg_chddr2_a', flags=False)\n",
    "heatdays = heatdays.iloc[:,:11].rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "## Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "target_cities = pd.read_csv(\"../data/Cities_with_codes.csv\")\n",
    "heatdays_nuts = pd.merge(target_cities, heatdays, on=[' NUTS 2'])\n",
    "\n",
    "## split into cool & heating days\n",
    "heatdays_cdd = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg == 'CDD')]\n",
    "heatdays_hdd = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg == 'HDD')]\n",
    "\n",
    "## reshape (melt) into city-year : value shape\n",
    "heat = heatdays_hdd.melt(id_vars=['City'], value_vars=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012],\\\n",
    "                         var_name = 'Year', value_name='Heatdays')\n",
    "cool = heatdays_cdd.melt(id_vars=['City'], value_vars=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012],\\\n",
    "                         var_name = 'Year', value_name='Cooldays')\n",
    "cool.shape\n",
    "#heatdays_cdd.to_csv('../data/heatdays_hdd.csv')\n",
    "#heatdays_hdd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(488, 4)\n",
      "(376, 5)\n",
      "(376, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Year</th>\n",
       "      <th>Heatdays</th>\n",
       "      <th>Cooldays</th>\n",
       "      <th>Pollution</th>\n",
       "      <th>QOL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>2019</td>\n",
       "      <td>2476.15</td>\n",
       "      <td>33.41</td>\n",
       "      <td>30.77</td>\n",
       "      <td>173.289993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antwerp</td>\n",
       "      <td>2019</td>\n",
       "      <td>2372.84</td>\n",
       "      <td>37.23</td>\n",
       "      <td>60.27</td>\n",
       "      <td>157.899994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>2019</td>\n",
       "      <td>1079.58</td>\n",
       "      <td>549.72</td>\n",
       "      <td>58.25</td>\n",
       "      <td>126.580002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Barcelona</td>\n",
       "      <td>2019</td>\n",
       "      <td>1799.98</td>\n",
       "      <td>238.14</td>\n",
       "      <td>65.83</td>\n",
       "      <td>141.850006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Valencia</td>\n",
       "      <td>2019</td>\n",
       "      <td>1799.98</td>\n",
       "      <td>238.14</td>\n",
       "      <td>43.76</td>\n",
       "      <td>178.009995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City  Year  Heatdays  Cooldays  Pollution         QOL\n",
       "0  Amsterdam  2019   2476.15     33.41      30.77  173.289993\n",
       "1    Antwerp  2019   2372.84     37.23      60.27  157.899994\n",
       "2     Athens  2019   1079.58    549.72      58.25  126.580002\n",
       "3  Barcelona  2019   1799.98    238.14      65.83  141.850006\n",
       "4   Valencia  2019   1799.98    238.14      43.76  178.009995"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "heatcool = pd.merge(heat, cool, how='inner', on=['City','Year'] )\n",
    "print(heatcool.shape)\n",
    "\n",
    "heatcoolpol = pd.merge(heatcool, pollution, how='inner', on=['City','Year'] )\n",
    "print(heatcoolpol.shape)\n",
    "\n",
    "heatcoolpolres = pd.merge(heatcoolpol, response , how='inner', on=['City','Year'] )\n",
    "print(heatcoolpolres.shape)\n",
    "heatcoolpolres.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Education Attainment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpolate with all object-dtype columns in the DataFrame. Try setting at least one column to a numeric dtype.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ea0e718203d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Impute the missing values from 2012 using padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0meducation_attainment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meducation_attainment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pad'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Split in to 3 separate tables depending on education level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[1;32m   7020\u001b[0m         ):\n\u001b[1;32m   7021\u001b[0m             raise TypeError(\n\u001b[0;32m-> 7022\u001b[0;31m                 \u001b[0;34m\"Cannot interpolate with all object-dtype columns \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7023\u001b[0m                 \u001b[0;34m\"in the DataFrame. Try setting at least one \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7024\u001b[0m                 \u001b[0;34m\"column to a numeric dtype.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpolate with all object-dtype columns in the DataFrame. Try setting at least one column to a numeric dtype."
     ]
    }
   ],
   "source": [
    "# Import the data from Eurostat\n",
    "df = eurostat.get_data_df('edat_lfse_04')\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df = df.drop(df.loc[:, '2011': '2000'].columns, axis = 1)\n",
    "df = df.drop(['unit', 'age'], axis = 1)\n",
    "\n",
    "# Keep only total values and discard the gender column\n",
    "df = df[df.sex == 'T']\n",
    "df = df.drop('sex', axis = 1)\n",
    "\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "target_cities = pd.read_csv(\"../data/Cities_with_codes.csv\")\n",
    "education_attainment = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = education_attainment.loc[education_attainment['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow (Metro area) is approx 33% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "education_attainment[education_attainment['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "\n",
    "# Impute the missing values from 2012 using padding\n",
    "education_attainment = education_attainment.interpolate(method='pad',axis=1)\n",
    "\n",
    "# Split in to 3 separate tables depending on education level\n",
    "edu_ED_0_2 = education_attainment[education_attainment['isced11'] == 'ED0-2']\n",
    "edu_ED_3_4 = education_attainment[education_attainment['isced11'] == 'ED3_4']\n",
    "edu_ED_5_8 = education_attainment[education_attainment['isced11'] == 'ED5-8']\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "education_attainment  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= edu_ED_0_2[ list(edu_ED_0_2.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Education Attainment ED 0-2\"})\n",
    "    yearly_data.insert(6, \"Education Attainment ED 3-4\", edu_ED_3_4[f\"{year}\"].to_numpy())\n",
    "    yearly_data.insert(7, \"Education Attainment ED 5-8\", edu_ED_5_8[f\"{year}\"].to_numpy())\n",
    "    education_attainment = education_attainment.append(yearly_data)\n",
    "    \n",
    "education_attainment = education_attainment.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Victims in Road Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing cities:  ['Belgrade' 'Edinburgh' 'Glasgow' 'Reykjavik']\n"
     ]
    }
   ],
   "source": [
    "# Get the data from Eurostat\n",
    "df = eurostat.get_data_df('tran_r_acci')\n",
    "df.columns = df.columns.astype(str)\n",
    "# Drop all years before 2012, keep only with unit measure Per Million Inhabitants and accident type deadly\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "df = df[df.unit == 'P_MHAB']\n",
    "df = df[df.victim == 'KIL']\n",
    "\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Insert missing cloumn for 2019\n",
    "df.insert(3,\"2019\",np.NaN)\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "target_cities = pd.read_csv(\"../data/Cities_with_codes.csv\")\n",
    "road_accidents = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Check for missing cities\n",
    "missing_cities = target_cities[-target_cities[' NUTS 2'].isin(road_accidents[' NUTS 2'])]\n",
    "\n",
    "\n",
    "print(\"Missing cities: \", missing_cities.values[:,0])\n",
    "\n",
    "# Data from Icelandic ministry of transportation\n",
    "#https://www.samgongustofa.is/umferd/tolfraedi/slysatolur/arsskyrslur-slysaskraningar/\n",
    "# Reported as deaths per 10,000\n",
    "Reykjavik  = [0.0, 30.0, 20.0, 10.0, 20.0, 0.0, 10.0, 10.0]\n",
    "Reykjavik = ['Reykjavik', 'IS001C1','IS00','IS','KIL','P_MHAB']+Reykjavik\n",
    "road_accidents.loc[77] = Reykjavik\n",
    "\n",
    "# Table 1-1 in : https://www.abs.gov.rs/admin/upload/documents/20181016102533-statistical_report_2016_english.pdf\n",
    "# 20% of the population is in Belgrade so we take 20% of the accidents as occuring there\n",
    "# and divide by 1.7 (million inhabitants)\n",
    "Belgrade = [np.NaN, np.NaN,np.NaN, 619, 594, 476, 548, 551]\n",
    "Belgrade = np.multiply(Belgrade,(0.2/1.7))\n",
    "Belgrade = np.around(Belgrade, 1)\n",
    "Belgrade = ['Belgrade', '-','RS11','RS','KIL','P_MHAB']+Belgrade.tolist()\n",
    "road_accidents.loc[78] = Belgrade\n",
    "\n",
    "# Traffic death data from Scotland\n",
    "# https://statistics.gov.scot/data/road-safety\n",
    "# Select by region\n",
    "\n",
    "# Divide by 0.5 (million inhabitants)\n",
    "Edinburgh = [np.NaN, 5, 6, 9, 3, 11, 8, 13]\n",
    "Edinburgh = np.divide(Edinburgh,0.5)\n",
    "Edinburgh = np.around(Edinburgh, 1)\n",
    "Edinburgh = ['Edinburgh', 'UK007C1','UKM7','UK','KIL','P_MHAB']+Edinburgh.tolist()\n",
    "road_accidents.loc[79] = Edinburgh\n",
    "\n",
    "# Divide by 0.6 (million inhabitants)\n",
    "Glasgow = [np.NaN, 10, 7, 8, 15, 18, 4, 7]\n",
    "Glasgow = np.divide(Glasgow,0.6)\n",
    "Glasgow = np.around(Glasgow, 1)\n",
    "Glasgow = ['Glasgow', 'UK004C1','UKM3','UK','KIL','P_MHAB']+Glasgow.tolist()\n",
    "road_accidents.loc[80] = Glasgow\n",
    "\n",
    "# Impute using padding to fill missing values\n",
    "road_accidents.loc[:, '2019':'2013'] = road_accidents.loc[:, '2019':'2012'].fillna(method='backfill',axis=1)\n",
    "road_accidents.loc[:, '2019':'2012'] = road_accidents.loc[:, '2019':'2012'].fillna(method='ffill',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "deaths_in_road_accidents  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= road_accidents[ list(road_accidents.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Deaths_in_road_accidents\"})\n",
    "    deaths_in_road_accidents = deaths_in_road_accidents.append(yearly_data)\n",
    "deaths_in_road_accidents = deaths_in_road_accidents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Working Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasheet from eurostat\n",
    "df = eurostat.get_data_df('lfst_r_lfe2ehour')\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df.columns = df.columns.astype(str)\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Extract the data for both sexes (Total) and age group 15-74\n",
    "df = df[df.sex == 'T']\n",
    "df = df.drop('sex', axis = 1)\n",
    "df = df[df.age == 'Y15-74']\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "target_cities = pd.read_csv(\"Cities_with_codes.csv\")\n",
    "avg_hours = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = avg_hours.loc[avg_hours['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow is 40% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "avg_hours[avg_hours['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "# Impute the rest using padding\n",
    "avg_hours = avg_hours.interpolate(method='pad',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "avg_working_hours  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= avg_hours[ list(avg_hours.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Average Working Hours\"})\n",
    "    avg_working_hours = avg_working_hours.append(yearly_data)\n",
    "avg_working_hours = avg_working_hours.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unemployment Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasheet from Eurostat\n",
    "df = eurostat.get_data_df('lfst_r_lfu3rt')\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df.columns = df.columns.astype(str)\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Extract the data for both sexes (Total) and age group 15-74\n",
    "df = df[df.sex == 'T']\n",
    "df = df[df.age == 'Y15-74']\n",
    "df = df.drop(['sex','unit','age'], axis = 1)\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "target_cities = pd.read_csv(\"Cities_with_codes.csv\")\n",
    "unemployment_rate = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = unemployment_rate.loc[unemployment_rate['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow is 40% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "unemployment_rate[unemployment_rate['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "# Impute the rest using padding\n",
    "unemployment_rate = unemployment_rate.interpolate(method='pad',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "unemployment_rate_data  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= unemployment_rate[ list(unemployment_rate.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Unemployment_Rate\"})\n",
    "    unemployment_rate_data = unemployment_rate_data.append(yearly_data)\n",
    "unemployment_rate_data = unemployment_rate_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code if we want to do it with csv files, requires all in the same format and not other existing csv files in the folder\n",
    "\n",
    "#initial = pd.read_csv('Education_Attainment.csv')\n",
    "#merged = initial\n",
    "#\n",
    "#for file_name in glob.glob('*.csv'):\n",
    "#    if file_name == 'Education_Attainment.csv':\n",
    "#        continue \n",
    "#    temp_dataframe = pd.read_csv(file_name)\n",
    "#    values = temp_dataframe.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "#    merged = merged.merge(values, on=['City', 'Year'])\n",
    "\n",
    "\n",
    "# Code to do it directly in the notebook without saving to csv files first\n",
    "# Right now requires all data to be complete\n",
    "data_sheets = [deaths_in_road_accidents,avg_working_hours,unemployment_rate_data]\n",
    "\n",
    "initial = education_attainment\n",
    "merged = initial\n",
    "\n",
    "for file_name in data_sheets:\n",
    "    temp_dataframe = file_name\n",
    "    values = temp_dataframe.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "    merged = merged.merge(values, on=['City', 'Year'])\n",
    "    \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "    \n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
