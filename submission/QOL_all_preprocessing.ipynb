{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Quality of Life \n",
    "### DOPP Group 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import eurostat\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response: Quality of Life ranking from Numbeo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Response_Variable import ReponseVariableScraper\n",
    "\n",
    "european_countries=[\"Switzerland\",\"Netherlands\",\"Denmark\",\n",
    "\"Austria\",\"Luxembourg\",\"Iceland\",\"United Kingdom\",\"Germany\",\n",
    "\"Spain\",\"Estonia\",\"Sweden\",\"Ireland\",\"Slovenia\",\"Lithuania\",\n",
    "        \"Turkey\",\"Czech Republic\",\"Norway\",\"Croatia\",\n",
    "        \"France\",\"Belgium\",\"Portugal\",\"Cyprus\",\"Romania\",\"Poland\",\"Slovakia\",\n",
    "        \"Latvia\",\"Russia\",\"Italy\",\"Bulgaria\",\"Serbia\",\"Greece\",\n",
    "\"Hungary\",\"Ukraine\"]\n",
    "\n",
    "def filter_europe(df):\n",
    "    df=df[df[\"Country\"].isin(european_countries)]\n",
    "    return df\n",
    "\n",
    "\n",
    "data_scraper = ReponseVariableScraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking & Score Aggregation\n",
    "After the necessary preprocessing has been done the next step in our time analysis is to summerize all the dataset into a dictionary containing the response variable, the ranking and city for every year.\n",
    "Furthermore, two a dataframe was created which counts for every city how often it scored in the top 10 by ranking during the time period 2012-2020. Additionally, a dataframe which accumulates for every\n",
    "city the scores that it received over the years. At last both dataframes were sorted by their counts of ranking or accumulated scores in descending order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "for year in range(2012,2021):\n",
    "    data[year]=data_scraper.get_year(year)\n",
    "    data[year]=filter_europe(data[year])\n",
    "    data[year][\"Rank\"]=range(1,len(data[year])+1)\n",
    "all_cities=[]\n",
    "for year in range(2012,2021):\n",
    "    all_cities.extend(data[year][\"City\"].unique())\n",
    "\n",
    "all_cities=np.unique(all_cities)\n",
    "cities=pd.DataFrame(columns=[\"City\",\"Top_10\"])\n",
    "cities[\"City\"]=all_cities\n",
    "cities[\"Top_10\"]=0\n",
    "QOL_acc=pd.DataFrame(columns=[\"City\",\"QOL_acc\"])\n",
    "QOL_acc[\"City\"]=all_cities\n",
    "QOL_acc[\"QOL_acc\"]=0\n",
    "for year in range(2012,2021):\n",
    "    df=data[year]\n",
    "    df.sort_values(by=\"City\",inplace=True)\n",
    "    candidates=df[df[\"Rank\"]<=10][\"City\"].to_list()\n",
    "    idx_c=cities[\"City\"].isin(candidates)\n",
    "    cities[\"Top_10\"].loc[idx_c]+=1\n",
    "    c=df[\"City\"].to_list()\n",
    "    idx_acc=QOL_acc[\"City\"].isin(c)\n",
    "    q=df[\"Quality of Life Index\"].to_list()\n",
    "    QOL_acc[\"QOL_acc\"][idx_acc]+=q\n",
    "cities.sort_values(by=[\"Top_10\"],ascending=False,inplace=True)\n",
    "\n",
    "qol_response = data_scraper.get_interpolated_years()\n",
    "\n",
    "\n",
    "## reshape (melt) into city-year : value shape, convert year to int to match others\n",
    "response = qol_response.melt(id_vars=['City'], value_vars=['2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012'],\\\n",
    "                         var_name = 'Year', value_name='QOL')\n",
    "response['Year'] = response['Year'].astype(int)\n",
    "response.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Pollution_Scraper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each year: read QOL + pollution csv, merge, add year, append to combined df \n",
    "pollution = pd.DataFrame()    \n",
    "for year in range(2012,2021):\n",
    "    ## read pollution for year\n",
    "    filename = ('../data/' + 'Pollution_' + str(year))\n",
    "    df_yr = pd.read_csv(filename).iloc[:,2:4]\n",
    "    \n",
    "    ## read Quality of Life for year\n",
    "    #filename_qol = ('../../data/' + 'Quality_of_life_' + str(year))\n",
    "    #df_qol = pd.read_csv(filename_qol).iloc[:,2:4]\n",
    "    \n",
    "    # merging QOL & Pollution \n",
    "    #df_yr = pd.merge(df_yr,qol_y, how = 'outer', on = 'City')\n",
    "    \n",
    "    ## define year and rename\n",
    "    df_yr.loc[:,'Year'] = int(year)\n",
    "    df_yr = df_yr.rename(columns = {'Pollution Index':'Pollution','Quality of Life Index':'QOL'})\n",
    "    \n",
    "    ## concat to combined df\n",
    "    pollution = pd.concat([pollution, df_yr])\n",
    "\n",
    "#df_all.to_csv('../data/Pol_merged.csv', index = False)\n",
    "pollution.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate: Cooling and heating degree days by NUTS 2 regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get eurostat data\n",
    "heatdays = eurostat.get_data_df('nrg_chddr2_a', flags=False)\n",
    "heatdays = heatdays.iloc[:,:11].rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "## Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "target_cities = pd.read_csv(\"../data/Cities_with_codes.csv\")\n",
    "heatdays_nuts = pd.merge(target_cities, heatdays, on=[' NUTS 2'])\n",
    "\n",
    "## split into cool & heating days\n",
    "heatdays_cdd = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg == 'CDD')]\n",
    "heatdays_hdd = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg == 'HDD')]\n",
    "\n",
    "## reshape (melt) into city-year : value shape\n",
    "heat = heatdays_hdd.melt(id_vars=['City'], value_vars=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012],\\\n",
    "                         var_name = 'Year', value_name='Heatdays')\n",
    "cool = heatdays_cdd.melt(id_vars=['City'], value_vars=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012],\\\n",
    "                         var_name = 'Year', value_name='Cooldays')\n",
    "cool.shape\n",
    "#heatdays_cdd.to_csv('../data/heatdays_hdd.csv')\n",
    "#heatdays_hdd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education Attainment\n",
    "From the eurostat dataset on education Population by educational attainment level (edat1) we use the sub-data set Population aged 25-64 by educational attainment level, sex and NUTS 2 regions (%) (edat_lfse_04). With educational attainment meaning the\n",
    "highest level of completed education, the data is divided into the three sub categories ED 0-2 (less than primary - primary education), ED 3-4 (lower secondary - post secondary education) and ED 5-8 (tertiary education i.e. university studies). Each city\n",
    "is given the values of the corresponding NUTS 2 region.\n",
    "\n",
    "Every city from the target variables are found represented in the\n",
    "data, however a few (['Belgrade', 'Budapest', 'Edinburgh', 'Kaunas', 'Vilnius', 'Warsaw']) have missing values for 2012.\n",
    "Since this data is the earliest value it is not possible to interpolate, therefore we simply pad with the 2013 value since this value should be closer than using the mean of all the years.\n",
    "Furthermore the data from Glasgow was almost completely missing except for 2012, but since Glasgow is a relatively large city in Scotland the population is close to 33% percent of the population\n",
    "of the whole of Scotland (the NUTS 1 region) which is available we fill it with these values as an approximation.\n",
    "\n",
    "Finally we separate the data into the three different brackets of educational level and drop the data for the total levels. This is since it should be\n",
    "a significant difference especially between the primary/secondary and the tertiary education since in general the first to\n",
    "are more heavliy regulated and handled on a governmental/regional compared to the Universities and similiar that usually are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Eurostat\n",
    "df = eurostat.get_data_df('edat_lfse_04')\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df = df.drop(df.loc[:, '2011': '2000'].columns, axis = 1)\n",
    "df = df.drop(['unit', 'age'], axis = 1)\n",
    "\n",
    "# Keep only total values and discard the gender column\n",
    "df = df[df.sex == 'T']\n",
    "df = df.drop('sex', axis = 1)\n",
    "\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "#target_cities = pd.read_csv(\"../data/Cities_with_codes.csv\")\n",
    "education_attainment = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = education_attainment.loc[education_attainment['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow (Metro area) is approx 33% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "education_attainment[education_attainment['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "\n",
    "# Impute the missing values from 2012 using padding\n",
    "education_attainment.loc[:, '2019':'2012'] = education_attainment.loc[:, '2019':'2012'].fillna(method='pad',axis=1)\n",
    "\n",
    "# Split in to 3 separate tables depending on education level\n",
    "edu_ED_0_2 = education_attainment[education_attainment['isced11'] == 'ED0-2']\n",
    "edu_ED_3_4 = education_attainment[education_attainment['isced11'] == 'ED3_4']\n",
    "edu_ED_5_8 = education_attainment[education_attainment['isced11'] == 'ED5-8']\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "education_attainment  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= edu_ED_0_2[ list(edu_ED_0_2.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Education Attainment ED 0-2\"})\n",
    "    yearly_data.insert(6, \"Education Attainment ED 3-4\", edu_ED_3_4[f\"{year}\"].to_numpy())\n",
    "    yearly_data.insert(7, \"Education Attainment ED 5-8\", edu_ED_5_8[f\"{year}\"].to_numpy())\n",
    "    education_attainment = education_attainment.append(yearly_data)\n",
    "    \n",
    "education_attainment = education_attainment.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Working Hours\n",
    "\n",
    "From the eurostat data Regional labour market statistics (reg_lmk) we use the dataset Average number of usual weekly hours of work in main job by sex, age and NUTS 2 regions (lfst_r_lfe2ehour), which contains data\n",
    "of working hours for a wide variety of different age categories and gender. We are however only interested in a total average\n",
    "for both sexes and all ages so we only use the largest agespan (15-74).\n",
    "\n",
    "The data is used as a measure of how much the average person\n",
    " needs to work every week which is clearly connected to the amount of leisure time which should have a large impact on the quality of life. The missing data is missing in the exact same spots as\n",
    "in the educational attainment data so we use the same approach here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the datasheet from eurostat\n",
    "df = eurostat.get_data_df('lfst_r_lfe2ehour')\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df.columns = df.columns.astype(str)\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Extract the data for both sexes (Total) and age group 15-74\n",
    "df = df[df.sex == 'T']\n",
    "df = df.drop('sex', axis = 1)\n",
    "df = df[df.age == 'Y15-74']\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "avg_hours = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = avg_hours.loc[avg_hours['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow is 40% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "avg_hours[avg_hours['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "# Impute the rest using padding\n",
    "#avg_hours = avg_hours.interpolate(method='pad',axis=1)\n",
    "avg_hours.loc[:, '2019':'2012'] = avg_hours.loc[:, '2019':'2012'].fillna(method='pad',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "avg_working_hours  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= avg_hours[ list(avg_hours.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Average Working Hours\"})\n",
    "    avg_working_hours = avg_working_hours.append(yearly_data)\n",
    "avg_working_hours = avg_working_hours.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment Rate\n",
    "\n",
    "From the eurostat data Regional labour market statistics (reg_lmk) we use the dataset \tUnemployment rates by sex, age and NUTS 2 regions (lfst_r_lfu3rt).\n",
    "Once again we are only interested in total statistics for the genders and the largest age-group and therefore use only the entries with sex 'T' and agespan 15-74.\n",
    "Also once again the same data is missing (likely these were all reported/obtained in the same survey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read the datasheet from Eurostat\n",
    "df = eurostat.get_data_df('lfst_r_lfu3rt')\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df.columns = df.columns.astype(str)\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Extract the data for both sexes (Total) and age group 15-74\n",
    "df = df[df.sex == 'T']\n",
    "df = df[df.age == 'Y15-74']\n",
    "df = df.drop(['sex','unit','age'], axis = 1)\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "unemployment_rate = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = unemployment_rate.loc[unemployment_rate['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow is 40% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "unemployment_rate[unemployment_rate['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "# Impute the rest using padding\n",
    "#unemployment_rate = unemployment_rate.interpolate(method='pad',axis=1)\n",
    "unemployment_rate.loc[:, '2019':'2012'] = unemployment_rate.loc[:, '2019':'2012'].fillna(method='pad',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "unemployment_rate_data  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= unemployment_rate[ list(unemployment_rate.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Unemployment_Rate\"})\n",
    "    unemployment_rate_data = unemployment_rate_data.append(yearly_data)\n",
    "unemployment_rate_data = unemployment_rate_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Victims in Road Accidents\n",
    "\n",
    "From the eurostat data Regional transport statistics (reg_tran) we use the dataset \tVictims in road accidents by NUTS 2 regions (tran_r_acc).\n",
    "The dataset contains data on a NUTS 2 level for reported road accidents. There are different measures availible, but we use P_MHAB (Per million inhabitants)\n",
    "as it gives a good way to compare the cities (with a wide variety of different sizes) to each other. There are also two different\n",
    "variables of reported accident: accidents with injuries or accidents with lethal outcome. Even though the injury data might\n",
    "be a better measure of the road safety in a city there is more data missing and hence we decided on the accidents with lethal outcome. Also the\n",
    "accidents with injueries there might be a larger dependency on willingness to report crimes than in the lethal accidents\n",
    "which are more likely very highly reported.\n",
    "\n",
    "Here there is also more data missing, there is no data at all for the cities\n",
    "Reykjavik, Edinburgh, Glasgow and Belgrade. Traffic data for these regions were obtained by searching up other data sources\n",
    "such as governmental reports from the corresponding countries and then approximated to the city. Also there is no data\n",
    "available at all for the year 2019 for which we pad the values from 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from Eurostat\n",
    "df = eurostat.get_data_df('tran_r_acci')\n",
    "df.columns = df.columns.astype(str)\n",
    "# Drop all years before 2012, keep only with unit measure Per Million Inhabitants and accident type deadly\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "df = df[df.unit == 'P_MHAB']\n",
    "df = df[df.victim == 'KIL']\n",
    "\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Insert missing cloumn for 2019\n",
    "df.insert(3,\"2019\",np.NaN)\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "road_accidents = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Check for missing cities\n",
    "missing_cities = target_cities[-target_cities[' NUTS 2'].isin(road_accidents[' NUTS 2'])]\n",
    "\n",
    "\n",
    "print(\"Missing cities: \", missing_cities.values[:,0])\n",
    "\n",
    "# Data from Icelandic ministry of transportation\n",
    "#https://www.samgongustofa.is/umferd/tolfraedi/slysatolur/arsskyrslur-slysaskraningar/\n",
    "# Reported as deaths per 10,000\n",
    "Reykjavik  = [0.0, 30.0, 20.0, 10.0, 20.0, 0.0, 10.0, 10.0]\n",
    "Reykjavik = ['Reykjavik', 'IS001C1','IS00','IS','KIL','P_MHAB']+Reykjavik\n",
    "road_accidents.loc[77] = Reykjavik\n",
    "\n",
    "# Table 1-1 in : https://www.abs.gov.rs/admin/upload/documents/20181016102533-statistical_report_2016_english.pdf\n",
    "# 20% of the population is in Belgrade so we take 20% of the accidents as occuring there\n",
    "# and divide by 1.7 (million inhabitants)\n",
    "Belgrade = [np.NaN, np.NaN,np.NaN, 619, 594, 476, 548, 551]\n",
    "Belgrade = np.multiply(Belgrade,(0.2/1.7))\n",
    "Belgrade = np.around(Belgrade, 1)\n",
    "Belgrade = ['Belgrade', '-','RS11','RS','KIL','P_MHAB']+Belgrade.tolist()\n",
    "road_accidents.loc[78] = Belgrade\n",
    "\n",
    "# Traffic death data from Scotland\n",
    "# https://statistics.gov.scot/data/road-safety\n",
    "# Select by region\n",
    "\n",
    "# Divide by 0.5 (million inhabitants)\n",
    "Edinburgh = [np.NaN, 5, 6, 9, 3, 11, 8, 13]\n",
    "Edinburgh = np.divide(Edinburgh,0.5)\n",
    "Edinburgh = np.around(Edinburgh, 1)\n",
    "Edinburgh = ['Edinburgh', 'UK007C1','UKM7','UK','KIL','P_MHAB']+Edinburgh.tolist()\n",
    "road_accidents.loc[79] = Edinburgh\n",
    "\n",
    "# Divide by 0.6 (million inhabitants)\n",
    "Glasgow = [np.NaN, 10, 7, 8, 15, 18, 4, 7]\n",
    "Glasgow = np.divide(Glasgow,0.6)\n",
    "Glasgow = np.around(Glasgow, 1)\n",
    "Glasgow = ['Glasgow', 'UK004C1','UKM3','UK','KIL','P_MHAB']+Glasgow.tolist()\n",
    "road_accidents.loc[80] = Glasgow\n",
    "\n",
    "# Impute using padding to fill missing values\n",
    "road_accidents.loc[:, '2019':'2013'] = road_accidents.loc[:, '2019':'2012'].fillna(method='backfill',axis=1)\n",
    "road_accidents.loc[:, '2019':'2012'] = road_accidents.loc[:, '2019':'2012'].fillna(method='ffill',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "deaths_in_road_accidents  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= road_accidents[ list(road_accidents.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Deaths_in_road_accidents\"})\n",
    "    deaths_in_road_accidents = deaths_in_road_accidents.append(yearly_data)\n",
    "deaths_in_road_accidents = deaths_in_road_accidents.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Education, Unemployemnt Rate, Accidents & Working Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code if we want to do it with csv files, requires all in the same format and not other existing csv files in the folder\n",
    "\n",
    "#initial = pd.read_csv('Education_Attainment.csv')\n",
    "#merged = initial\n",
    "#\n",
    "#for file_name in glob.glob('*.csv'):\n",
    "#    if file_name == 'Education_Attainment.csv':\n",
    "#        continue \n",
    "#    temp_dataframe = pd.read_csv(file_name)\n",
    "#    values = temp_dataframe.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "#    merged = merged.merge(values, on=['City', 'Year'])\n",
    "\n",
    "\n",
    "# Code to do it directly in the notebook without saving to csv files first\n",
    "# Right now requires all data to be complete\n",
    "data_sheets = [deaths_in_road_accidents,avg_working_hours,unemployment_rate_data]\n",
    "\n",
    "initial = education_attainment\n",
    "merged = initial\n",
    "\n",
    "for file_name in data_sheets:\n",
    "    temp_dataframe = file_name\n",
    "    values = temp_dataframe.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "    merged = merged.merge(values, on=['City', 'Year'])\n",
    "    \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "    \n",
    "print(merged.shape)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health and Gender\n",
    "\n",
    "Unfortunately, parts of the health data could not be retrieved using the scrapper. We thus downloaded it to a .csv file manually from the website (https://ec.europa.eu/eurostat/databrowser/view/hlth_rs_prsrg/default/table?lang=en). On a NUTS2 level the health variables available are life expectancy and medical staff. The later is subdivided (There seems to be the problem with the scrapper.) into absolute and relative number as well as certain classes of personnel: Dentists, medical doctors, midwifes, etc. We chose the variable 'medical doctors' and decided to use the relative number per 100.000 inhabitants as the observed cities vary largely in population. Pre-processing has been necessary for missing values. First, linear interpolation has been used to calculate single missing values. Second, if consecutive values have been missing a simple regression has been applied to calculate the missing values. Third, if no data has been available for a given city at all, we had to resort to the next more crude granularity to use that data instead which had to be applied for example for Warsaw. Last, for all cities of the United Kingdom and Ireland data on medical personnel has neither been reported on a NUTS2 nor on a NUTS1 level. We thus had to use national data to fill in the gaps. As a result all English, Scottish and northern Irish have the same number of medical doctors per 100.000 inhabitants. The same goes for the Irish cities Cork and Dublin. \n",
    "\n",
    "Additionally the variable 'gender unemployment gap' has been chosen to approximate the level of equality. It is the difference between male and female unemployment rate and monitors female labor market access. It should be corrected for overall unemployment rate to take economic development into consideration. So this might not be the best variable to measure gender inequality, but it is the only one on a regional level we could find. The data has been available for all cities except for Belgrade which we approximated using the average of the neighboring capitals Budapest, Bucharest and Sofia for each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_gender = pd.read_csv('../data/Cities_Health_Gender.csv')\n",
    "health_gender = health_gender.drop(columns=[' City_Code', ' NUTS 2',' Country'])\n",
    "#health_gender.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = 'inner'\n",
    "#join = 'outer'\n",
    "\n",
    "heatcool = pd.merge(heat, cool, how=join, on=['City','Year'] )\n",
    "print(heatcool.shape)\n",
    "\n",
    "heatcoolpol = pd.merge(heatcool, pollution, how=join, on=['City','Year'] )\n",
    "print(heatcoolpol.shape)\n",
    "\n",
    "tmppolres = pd.merge(heatcoolpol, response , how=join, on=['City','Year'] )\n",
    "print(tmppolres.shape)\n",
    "tmppolres.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "halfdata = pd.merge(tmppolres, merged , how=join, on=['City','Year'] )\n",
    "halfdata = halfdata.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "print(halfdata.shape)\n",
    "#halfdata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.merge(halfdata, health_gender, how=join, on=['City','Year'] )\n",
    "print(alldata.shape)\n",
    "alldata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inner shape: (376, 15), outer: (1072, 15) \n",
    "if join == 'inner':\n",
    "    alldata.to_csv('../data/Full_data_inner.csv', index = False)\n",
    "elif join == 'outer':\n",
    "    alldata.to_csv('../data/Full_data_outer.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inner = pd.read_csv('../data/Full_data_inner.csv')\n",
    "all_inner.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
