{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City Quality of Life \n",
    "### DOPP Group 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import eurostat\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response: Quality of Life ranking from Numbeo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Response_Variable import ReponseVariableScraper\n",
    "\n",
    "european_countries=[\"Switzerland\",\"Netherlands\",\"Denmark\",\n",
    "\"Austria\",\"Luxembourg\",\"Iceland\",\"United Kingdom\",\"Germany\",\n",
    "\"Spain\",\"Estonia\",\"Sweden\",\"Ireland\",\"Slovenia\",\"Lithuania\",\n",
    "        \"Turkey\",\"Czech Republic\",\"Norway\",\"Croatia\",\n",
    "        \"France\",\"Belgium\",\"Portugal\",\"Cyprus\",\"Romania\",\"Poland\",\"Slovakia\",\n",
    "        \"Latvia\",\"Russia\",\"Italy\",\"Bulgaria\",\"Serbia\",\"Greece\",\n",
    "\"Hungary\",\"Ukraine\"]\n",
    "\n",
    "def filter_europe(df):\n",
    "    df=df[df[\"Country\"].isin(european_countries)]\n",
    "    return df\n",
    "\n",
    "\n",
    "data_scraper = ReponseVariableScraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking & Score Aggregation\n",
    "After the necessary preprocessing has been done the next step in our time analysis is to summerize all the dataset into a dictionary containing the response variable, the ranking and city for every year.\n",
    "Furthermore, two a dataframe was created which counts for every city how often it scored in the top 10 by ranking during the time period 2012-2020. Additionally, a dataframe which accumulates for every\n",
    "city the scores that it received over the years. At last both dataframes were sorted by their counts of ranking or accumulated scores in descending order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data={}\n",
    "for year in range(2012,2021):\n",
    "    data[year]=data_scraper.get_year(year)\n",
    "    data[year]=filter_europe(data[year])\n",
    "    data[year][\"Rank\"]=range(1,len(data[year])+1)\n",
    "all_cities=[]\n",
    "for year in range(2012,2021):\n",
    "    all_cities.extend(data[year][\"City\"].unique())\n",
    "\n",
    "all_cities=np.unique(all_cities)\n",
    "cities=pd.DataFrame(columns=[\"City\",\"Top_10\"])\n",
    "cities[\"City\"]=all_cities\n",
    "cities[\"Top_10\"]=0\n",
    "QOL_acc=pd.DataFrame(columns=[\"City\",\"QOL_acc\"])\n",
    "QOL_acc[\"City\"]=all_cities\n",
    "QOL_acc[\"QOL_acc\"]=0\n",
    "for year in range(2012,2021):\n",
    "    df=data[year]\n",
    "    df.sort_values(by=\"City\",inplace=True)\n",
    "    candidates=df[df[\"Rank\"]<=10][\"City\"].to_list()\n",
    "    idx_c=cities[\"City\"].isin(candidates)\n",
    "    cities[\"Top_10\"].loc[idx_c]+=1\n",
    "    c=df[\"City\"].to_list()\n",
    "    idx_acc=QOL_acc[\"City\"].isin(c)\n",
    "    q=df[\"Quality of Life Index\"].to_list()\n",
    "    QOL_acc[\"QOL_acc\"][idx_acc]+=q\n",
    "cities.sort_values(by=[\"Top_10\"],ascending=False,inplace=True)\n",
    "\n",
    "qol_response = data_scraper.get_interpolated_years()\n",
    "\n",
    "\n",
    "## reshape (melt) into city-year : value shape, convert year to int to match others\n",
    "response = qol_response.melt(id_vars=['City'], value_vars=['2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012'],\\\n",
    "                         var_name = 'Year', value_name='QOL')\n",
    "response['Year'] = response['Year'].astype(int)\n",
    "response.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pollution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python Pollution_Scraper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each year: read QOL + pollution csv, merge, add year, append to combined df \n",
    "pollution = pd.DataFrame()    \n",
    "for year in range(2012,2021):\n",
    "    ## read pollution for year\n",
    "    filename = ('../data/' + 'Pollution_' + str(year))\n",
    "    df_yr = pd.read_csv(filename).iloc[:,2:4]\n",
    "    \n",
    "    ## read Quality of Life for year\n",
    "    #filename_qol = ('../../data/' + 'Quality_of_life_' + str(year))\n",
    "    #df_qol = pd.read_csv(filename_qol).iloc[:,2:4]\n",
    "    \n",
    "    # merging QOL & Pollution \n",
    "    #df_yr = pd.merge(df_yr,qol_y, how = 'outer', on = 'City')\n",
    "    \n",
    "    ## define year and rename\n",
    "    df_yr.loc[:,'Year'] = int(year)\n",
    "    df_yr = df_yr.rename(columns = {'Pollution Index':'Pollution','Quality of Life Index':'QOL'})\n",
    "    \n",
    "    ## concat to combined df\n",
    "    pollution = pd.concat([pollution, df_yr])\n",
    "\n",
    "#df_all.to_csv('../data/Pol_merged.csv', index = False)\n",
    "pollution.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate: Cooling and heating degree days by NUTS 2 regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get eurostat data\n",
    "heatdays = eurostat.get_data_df('nrg_chddr2_a', flags=False)\n",
    "heatdays = heatdays.iloc[:,:11].rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "## Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "target_cities = pd.read_csv(\"../data/Cities_with_codes.csv\")\n",
    "heatdays_nuts = pd.merge(target_cities, heatdays, on=[' NUTS 2'])\n",
    "\n",
    "## split into cool & heating days\n",
    "heatdays_cdd = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg == 'CDD')]\n",
    "heatdays_hdd = heatdays_nuts.iloc[np.where(heatdays_nuts.indic_nrg == 'HDD')]\n",
    "\n",
    "## reshape (melt) into city-year : value shape\n",
    "heat = heatdays_hdd.melt(id_vars=['City'], value_vars=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012],\\\n",
    "                         var_name = 'Year', value_name='Heatdays')\n",
    "cool = heatdays_cdd.melt(id_vars=['City'], value_vars=[2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012],\\\n",
    "                         var_name = 'Year', value_name='Cooldays')\n",
    "cool.shape\n",
    "#heatdays_cdd.to_csv('../data/heatdays_hdd.csv')\n",
    "#heatdays_hdd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education Attainment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Eurostat\n",
    "df = eurostat.get_data_df('edat_lfse_04')\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df = df.drop(df.loc[:, '2011': '2000'].columns, axis = 1)\n",
    "df = df.drop(['unit', 'age'], axis = 1)\n",
    "\n",
    "# Keep only total values and discard the gender column\n",
    "df = df[df.sex == 'T']\n",
    "df = df.drop('sex', axis = 1)\n",
    "\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "#target_cities = pd.read_csv(\"../data/Cities_with_codes.csv\")\n",
    "education_attainment = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = education_attainment.loc[education_attainment['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow (Metro area) is approx 33% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "education_attainment[education_attainment['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "\n",
    "# Impute the missing values from 2012 using padding\n",
    "education_attainment.loc[:, '2019':'2012'] = education_attainment.loc[:, '2019':'2012'].fillna(method='pad',axis=1)\n",
    "\n",
    "# Split in to 3 separate tables depending on education level\n",
    "edu_ED_0_2 = education_attainment[education_attainment['isced11'] == 'ED0-2']\n",
    "edu_ED_3_4 = education_attainment[education_attainment['isced11'] == 'ED3_4']\n",
    "edu_ED_5_8 = education_attainment[education_attainment['isced11'] == 'ED5-8']\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "education_attainment  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= edu_ED_0_2[ list(edu_ED_0_2.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Education Attainment ED 0-2\"})\n",
    "    yearly_data.insert(6, \"Education Attainment ED 3-4\", edu_ED_3_4[f\"{year}\"].to_numpy())\n",
    "    yearly_data.insert(7, \"Education Attainment ED 5-8\", edu_ED_5_8[f\"{year}\"].to_numpy())\n",
    "    education_attainment = education_attainment.append(yearly_data)\n",
    "    \n",
    "education_attainment = education_attainment.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Victims in Road Accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from Eurostat\n",
    "df = eurostat.get_data_df('tran_r_acci')\n",
    "df.columns = df.columns.astype(str)\n",
    "# Drop all years before 2012, keep only with unit measure Per Million Inhabitants and accident type deadly\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "df = df[df.unit == 'P_MHAB']\n",
    "df = df[df.victim == 'KIL']\n",
    "\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Insert missing cloumn for 2019\n",
    "df.insert(3,\"2019\",np.NaN)\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "road_accidents = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Check for missing cities\n",
    "missing_cities = target_cities[-target_cities[' NUTS 2'].isin(road_accidents[' NUTS 2'])]\n",
    "\n",
    "\n",
    "print(\"Missing cities: \", missing_cities.values[:,0])\n",
    "\n",
    "# Data from Icelandic ministry of transportation\n",
    "#https://www.samgongustofa.is/umferd/tolfraedi/slysatolur/arsskyrslur-slysaskraningar/\n",
    "# Reported as deaths per 10,000\n",
    "Reykjavik  = [0.0, 30.0, 20.0, 10.0, 20.0, 0.0, 10.0, 10.0]\n",
    "Reykjavik = ['Reykjavik', 'IS001C1','IS00','IS','KIL','P_MHAB']+Reykjavik\n",
    "road_accidents.loc[77] = Reykjavik\n",
    "\n",
    "# Table 1-1 in : https://www.abs.gov.rs/admin/upload/documents/20181016102533-statistical_report_2016_english.pdf\n",
    "# 20% of the population is in Belgrade so we take 20% of the accidents as occuring there\n",
    "# and divide by 1.7 (million inhabitants)\n",
    "Belgrade = [np.NaN, np.NaN,np.NaN, 619, 594, 476, 548, 551]\n",
    "Belgrade = np.multiply(Belgrade,(0.2/1.7))\n",
    "Belgrade = np.around(Belgrade, 1)\n",
    "Belgrade = ['Belgrade', '-','RS11','RS','KIL','P_MHAB']+Belgrade.tolist()\n",
    "road_accidents.loc[78] = Belgrade\n",
    "\n",
    "# Traffic death data from Scotland\n",
    "# https://statistics.gov.scot/data/road-safety\n",
    "# Select by region\n",
    "\n",
    "# Divide by 0.5 (million inhabitants)\n",
    "Edinburgh = [np.NaN, 5, 6, 9, 3, 11, 8, 13]\n",
    "Edinburgh = np.divide(Edinburgh,0.5)\n",
    "Edinburgh = np.around(Edinburgh, 1)\n",
    "Edinburgh = ['Edinburgh', 'UK007C1','UKM7','UK','KIL','P_MHAB']+Edinburgh.tolist()\n",
    "road_accidents.loc[79] = Edinburgh\n",
    "\n",
    "# Divide by 0.6 (million inhabitants)\n",
    "Glasgow = [np.NaN, 10, 7, 8, 15, 18, 4, 7]\n",
    "Glasgow = np.divide(Glasgow,0.6)\n",
    "Glasgow = np.around(Glasgow, 1)\n",
    "Glasgow = ['Glasgow', 'UK004C1','UKM3','UK','KIL','P_MHAB']+Glasgow.tolist()\n",
    "road_accidents.loc[80] = Glasgow\n",
    "\n",
    "# Impute using padding to fill missing values\n",
    "road_accidents.loc[:, '2019':'2013'] = road_accidents.loc[:, '2019':'2012'].fillna(method='backfill',axis=1)\n",
    "road_accidents.loc[:, '2019':'2012'] = road_accidents.loc[:, '2019':'2012'].fillna(method='ffill',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "deaths_in_road_accidents  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= road_accidents[ list(road_accidents.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Deaths_in_road_accidents\"})\n",
    "    deaths_in_road_accidents = deaths_in_road_accidents.append(yearly_data)\n",
    "deaths_in_road_accidents = deaths_in_road_accidents.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Working Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasheet from eurostat\n",
    "df = eurostat.get_data_df('lfst_r_lfe2ehour')\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df.columns = df.columns.astype(str)\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Extract the data for both sexes (Total) and age group 15-74\n",
    "df = df[df.sex == 'T']\n",
    "df = df.drop('sex', axis = 1)\n",
    "df = df[df.age == 'Y15-74']\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "avg_hours = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = avg_hours.loc[avg_hours['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow is 40% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "avg_hours[avg_hours['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "# Impute the rest using padding\n",
    "#avg_hours = avg_hours.interpolate(method='pad',axis=1)\n",
    "avg_hours.loc[:, '2019':'2012'] = avg_hours.loc[:, '2019':'2012'].fillna(method='pad',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "avg_working_hours  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= avg_hours[ list(avg_hours.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Average Working Hours\"})\n",
    "    avg_working_hours = avg_working_hours.append(yearly_data)\n",
    "avg_working_hours = avg_working_hours.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployment Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasheet from Eurostat\n",
    "df = eurostat.get_data_df('lfst_r_lfu3rt')\n",
    "\n",
    "# Drop all years before 2012 and columns unit and age (same for all entries)\n",
    "df.columns = df.columns.astype(str)\n",
    "df = df.drop(df.loc[:, '2011': ].columns, axis = 1)\n",
    "# Rename to avoid problems using \\\n",
    "df = df.rename(columns={'geo\\\\time': ' NUTS 2'})\n",
    "\n",
    "# Extract the data for both sexes (Total) and age group 15-74\n",
    "df = df[df.sex == 'T']\n",
    "df = df[df.age == 'Y15-74']\n",
    "df = df.drop(['sex','unit','age'], axis = 1)\n",
    "\n",
    "# Merge on all entries which are also in the target variable cities to extract only the interesting cities\n",
    "unemployment_rate = pd.merge(target_cities, df, on=[' NUTS 2'])\n",
    "\n",
    "# Glasgow is missing all values except 2012\n",
    "Glasgow = unemployment_rate.loc[unemployment_rate['City'] == 'Glasgow']\n",
    "# Whole region of Scotland is UKM\n",
    "# Glasgow is 40% of the population it should be a reasonable approximation\n",
    "temp  = df[df[' NUTS 2'] == 'UKM']\n",
    "temp = temp.loc[:,'2019':'2013']\n",
    "Glasgow.loc[:, '2019':'2013'] = temp.loc[:, '2019':'2013'].to_numpy()\n",
    "unemployment_rate[unemployment_rate['City'] == 'Glasgow'] = Glasgow\n",
    "\n",
    "# Impute the rest using padding\n",
    "#unemployment_rate = unemployment_rate.interpolate(method='pad',axis=1)\n",
    "unemployment_rate.loc[:, '2019':'2012'] = unemployment_rate.loc[:, '2019':'2012'].fillna(method='pad',axis=1)\n",
    "\n",
    "#To be able to merge with the other data\n",
    "#Transform the columns of each year to a variable year\n",
    "yearly_data = dict()\n",
    "unemployment_rate_data  = pd.DataFrame()\n",
    "for year in range(2012,2020):\n",
    "    yearly_data= unemployment_rate[ list(unemployment_rate.loc[:,'City':' Country']) + [f\"{year}\"]]\n",
    "    yearly_data.insert(4, \"Year\", year)\n",
    "    yearly_data = yearly_data.rename(columns={f\"{year}\": \"Unemployment_Rate\"})\n",
    "    unemployment_rate_data = unemployment_rate_data.append(yearly_data)\n",
    "unemployment_rate_data = unemployment_rate_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Education, Accidents & Working Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code if we want to do it with csv files, requires all in the same format and not other existing csv files in the folder\n",
    "\n",
    "#initial = pd.read_csv('Education_Attainment.csv')\n",
    "#merged = initial\n",
    "#\n",
    "#for file_name in glob.glob('*.csv'):\n",
    "#    if file_name == 'Education_Attainment.csv':\n",
    "#        continue \n",
    "#    temp_dataframe = pd.read_csv(file_name)\n",
    "#    values = temp_dataframe.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "#    merged = merged.merge(values, on=['City', 'Year'])\n",
    "\n",
    "\n",
    "# Code to do it directly in the notebook without saving to csv files first\n",
    "# Right now requires all data to be complete\n",
    "data_sheets = [deaths_in_road_accidents,avg_working_hours,unemployment_rate_data]\n",
    "\n",
    "initial = education_attainment\n",
    "merged = initial\n",
    "\n",
    "for file_name in data_sheets:\n",
    "    temp_dataframe = file_name\n",
    "    values = temp_dataframe.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "    merged = merged.merge(values, on=['City', 'Year'])\n",
    "    \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "    \n",
    "print(merged.shape)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join = 'inner'\n",
    "#join = 'outer'\n",
    "\n",
    "heatcool = pd.merge(heat, cool, how=join, on=['City','Year'] )\n",
    "print(heatcool.shape)\n",
    "\n",
    "heatcoolpol = pd.merge(heatcool, pollution, how=join, on=['City','Year'] )\n",
    "print(heatcoolpol.shape)\n",
    "\n",
    "heatcoolpolres = pd.merge(heatcoolpol, response , how=join, on=['City','Year'] )\n",
    "print(heatcoolpolres.shape)\n",
    "heatcoolpolres.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.merge(heatcoolpolres, merged , how=join, on=['City','Year'] )\n",
    "alldata = alldata.drop(columns=[' City Code', ' NUTS 2',' Country'])\n",
    "print(alldata.shape)\n",
    "alldata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata.to_csv('../data/Full_data_merged.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# using join = 'outer' to merge\n",
    "# alldata.to_csv('../data/Full_data_outer.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inner = pd.read_csv('../data/Full_data_merged.csv')\n",
    "all_inner.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
